# -*- coding: utf-8 -*-
"""25212025_churning_customers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqBh-6xuy3UUolA9J7OOcH0GiivgXb8_

##**Importing my drive and all necessary modules**
"""

try:
    import scikeras
except ImportError:
    !pip install scikeras


try:
    import researchpy as rp
except ImportError:
    !pip install researchpy

import researchpy as rp
from google.colab import drive
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from keras.models import Sequential
from keras.layers import Dense
import keras
from keras.models import Model
from imblearn.over_sampling import RandomOverSampler
import pickle
from scikeras.wrappers import KerasClassifier
from keras.models import Model
from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation
from keras.optimizers import Adam, SGD
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.utils import class_weight

"""##**Mounting the drive**

"""

drive.mount('/content/drive')

"""###**Reading the .csv file**

To read the CSV file, I also want to store it in a data frame. So in order for me to store it in a data frame, I would first import the pandas module.
"""

import pandas as pd

customers = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')

"""#**Now I would check to see the structure of my whole dataset**

#**Understanding the relationship between the variables in my dataset**
"""

variables_to_draw = []
for col in customers.columns:
    if customers.dtypes[col] == 'object':
        variables_to_draw.append(col)

variables_to_draw.remove('customerID')
variables_to_draw.remove('TotalCharges')
variables_to_draw.remove('Churn')
distributions_to_focus_on = customers[variables_to_draw].copy()
distributions_to_focus_on

for var in variables_to_draw:
    churn_rate = customers.groupby(var)['Churn'].value_counts(normalize=True).unstack()
    churn_rate = churn_rate['Yes']  # Assuming 'Yes' is the label for churned customers

    plt.figure(figsize=(8, 4))
    sns.barplot(x=churn_rate.index, y=churn_rate.values)
    plt.title(f'Churn Rate by {var}')
    plt.ylabel('Churn Rate')
    plt.show()

# Assuming 'variables_to_draw' is a list of your variables and 'customers' is your DataFrame
num_variables = len(variables_to_draw)
n_rows = (num_variables + 1) // 2  # Determine the number of rows needed for the subplots
fig, axes = plt.subplots(n_rows, 2, figsize=(18, n_rows * 6))  # Adjust the size as needed

# Flatten the axes array for easy iteration
axes = axes.flatten()

for i, feature in enumerate(variables_to_draw):
    sns.countplot(x=feature, hue='Churn', data=customers, palette='viridis', alpha=0.5, ax=axes[i])
    axes[i].set_title(f'{feature} vs Churn')
    axes[i].set_xlabel('Feature')
    axes[i].set_ylabel('Count')
    axes[i].legend(title='Churn', loc='upper right')

# Hide any empty subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.title('Variables vs Churn', fontsize=8)
plt.tight_layout()
plt.show()

contingency_table = pd.crosstab(customers['Churn'], customers['gender'])
contingency_table.plot(kind='bar', stacked=True)
plt.show()

plt.figure(figsize=(5,5))
customers.Churn.value_counts().plot(kind='pie',autopct='%1.1f',explode=(0.1,0))

contingency_table = pd.crosstab(customers['Churn'], customers['Partner'])
contingency_table.plot(kind='bar', stacked=True)
plt.show()

contingency_table = pd.crosstab(customers['Churn'], customers['SeniorCitizen'])
contingency_table.plot(kind='bar', stacked=True)
plt.show()

import seaborn as sns

# Visualizing relationships between features
sns.pairplot(data=customers, hue='Churn')

non_categorical_variable_to_use = []
for col in customers.columns:
    if customers.dtypes[col] == 'int64':
      non_categorical_variable_to_use.append(col)
    elif customers.dtypes[col] == 'float64':
      non_categorical_variable_to_use.append(col)

numerical_customers_churn = customers[non_categorical_variable_to_use].copy()
numerical_customers_churn['Churn'] = customers['Churn']
numerical_customers_churn['Churn'] = numerical_customers_churn['Churn'].map({'Yes': 1, 'No': 0})

for Col in non_categorical_variable_to_use:
    plt.figure(figsize=(8, 4))
    sns.kdeplot(numerical_customers_churn[numerical_customers_churn['Churn'] == 1][Col], label='Churned', shade=True)
    sns.kdeplot(numerical_customers_churn[numerical_customers_churn['Churn'] == 0][Col], label='Not Churned', shade=True)
    plt.title(f'Distribution of {Col} for Churned vs. Not Churned Customers')
    plt.legend()
    plt.show()

customers.head()

customers.info()

number_of_empty_strings = 0
for column in customers.columns:
    # Loop through each row in the column
    for index, value in customers[column].items():
        # Check if the value is an empty string
        if value == " ":
            # Increment the counter if an empty string is found
            number_of_empty_strings += 1
print(f"The number of empty strings that can be found in this dataset is {number_of_empty_strings}")

"""Since there are some empty strings in my dataset, I would now try to fix these empty strings by replacing it with NaN"""

from numpy import NaN
for column in customers.columns:
    customers[column].replace(" ", np.NaN, inplace=True)

customers.info()

customers.describe

"""#**Now I would carry out my exploratory data analysis**

From studying the entire dataset, it can be concluded that the entire TotalCharges column is typically a float. However, its type is an object. So to make it much easier to cater for the lost values, I would now change all the the type of the TotalCharges to float
"""

customers['TotalCharges'] = customers['TotalCharges'].astype(float)

customers.info()

"""Seeing as in this dataframe, there is no column where over 30% of the data were missing values. I can proceed to work with all the columns in the dataset."""

customers['TotalCharges'].interpolate(inplace=True)

customers.info()

non_categorical_variables = []
for col in customers.columns:
    if customers.dtypes[col] == 'int64':
      non_categorical_variables.append(col)
    elif customers.dtypes[col] == 'float64':
      non_categorical_variables.append(col)

for col in non_categorical_variables:
  print(col)

customers_churn_numerical = customers[non_categorical_variables].copy()
customers_churn_numerical['Churn'] = customers['Churn']
customers_churn_numerical['Churn'] = customers_churn_numerical['Churn'].map({'Yes': 1, 'No': 0})
customers_churn_numerical

customers_churn = customers.copy()

customers_churn['Profile'] = customers['SeniorCitizen'].map({0: 'Not a Senior', 1: "Is a Senior"}) + ' ' + customers['gender'] + 'has a partner, and dependents' + customers['Dependents']

"""#**Checks**"""

columns_to_encode = []
for col in customers.columns:
    if customers.dtypes[col] == 'object':
        columns_to_encode.append(col)

categorical_values = customers[columns_to_encode].copy()
categorical_values.drop('customerID', inplace=True, axis=1)
categorical_values

new_customers = customers.copy()
new_customers = new_customers.drop(columns="customerID")

new_customers.drop(columns = categorical_values, inplace = True )
new_customers

label_encoder = LabelEncoder()
for column in categorical_values.columns:
    categorical_values[column] = label_encoder.fit_transform(categorical_values[column])

# Impute missing values
imp = SimpleImputer(strategy="most_frequent")
imputed_data = imp.fit_transform(categorical_values)

# Create a DataFrame with the imputed data
encoded_categorical = pd.DataFrame(imputed_data, columns=categorical_values.columns)

# Display the first 5 rows of the encoded DataFrame
print(encoded_categorical.head(5))

encoded_customers = pd.concat([new_customers, encoded_categorical], axis = 1)
encoded_customers

y = encoded_customers["Churn"]
X = encoded_customers.drop("Churn", axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelrf = RandomForestClassifier(n_estimators=100, random_state=42)
modelrf.fit(X_train, y_train)

X_train.shape

encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(X_train)
importances = modelrf.feature_importances_
feature_names = encoder.get_feature_names_out(X_train.columns)
# Sort them in descending order
indices = np.argsort(importances)[::-1]

"Let's display the ranking of feature importance."
print("Feature ranking:")

for i in range(X.shape[1]):
    print(f"{i + 1} {X.columns[indices[i]]} ({importances[indices[i]]})")

plt.title('Feature Importance')
sorted_indices = np.argsort(importances)[::-1]
plt.bar(range(X.shape[1]), importances[sorted_indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[sorted_indices], rotation=90)
plt.tight_layout()
plt.show()

# Get the top 15 most important features
top_features_number = 15
top_features = [X.columns[indices[i]] for i in range(top_features_number)]

# Create a DataFrame with the top features from 'encoded_players21'
customers_churned = encoded_customers[top_features]
customers_churned

sc = StandardScaler()

X = sc.fit_transform(customers_churned)
X = pd.DataFrame(X,columns = customers_churned.columns)
y = encoded_customers["Churn"]
X

"""#**Model Training**"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Applying SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

def create_model(input_shape):
    # Define the input layer
    input_layer = Input(shape=(input_shape,))
    # Define the hidden layers
    x = Dense(64, activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(64, activation='relu')(x)
    x = BatchNormalization()(x)
    # Define the output layer
    output_layer = Dense(1, activation='sigmoid')(x)
    # Create the model
    model = Model(inputs=input_layer, outputs=output_layer)
    return model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

input_shape = (X_train.shape[1])
input_shape

mlp_model = create_model(input_shape)

# Compile the model
optimizer = Adam(learning_rate=0.001)
mlp_model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
mlp_model.fit(X_train, y_train,
                        batch_size=32,
                        epochs=50,
                        validation_split=0.2,
                        verbose=1)  # Set to 1 to see the progress bar

# Evaluate the model on the test set
y_pred = (mlp_model.predict(X_test) > 0.5).astype("int32")
test_accuracy = accuracy_score(y_test, y_pred)
test_accuracy

y_prob = mlp_model.predict(X_test)
auc_score = roc_auc_score(y_test, y_prob)

print("AUC Score:", auc_score)

# Calculate class weights
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

# Define the MLP model
def create_model(neurons=32, learning_rate=0.001, optimizer='adam', dropout_rate=0.5):
    input_layer = Input(shape=(X_train.shape[1],))
    x = Dense(neurons)(input_layer)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(dropout_rate)(x)
    output_layer = Dense(1, activation='sigmoid')(x)

    # Choose optimizer
    if optimizer == 'adam':
        opt = Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        opt = SGD(learning_rate=learning_rate)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    #history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, class_weight=class_weights)
    #return model, history
    return model


# Wrap the model with KerasClassifier
model = KerasClassifier(model=create_model, verbose=0)

# Define the grid search parameters
param_grid = {
    'model__neurons': [32, 64, 128],
    'model__learning_rate': [0.001, 0.01, 0.1],
    'model__optimizer': ['adam', 'sgd'],
    'model__dropout_rate': [0.3, 0.5, 0.7],
    'epochs': [50, 100],
    'batch_size': [64, 128]
}

# Create StratifiedKFold for cross-validation
kfold_splits = 5
kfold = StratifiedKFold(n_splits=kfold_splits, shuffle=True, random_state=42)

# Create GridSearchCV
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=kfold, scoring='roc_auc')

# Fit the GridSearchCV (no need to pass class_weight directly here)
grid_result = grid.fit(X_train, y_train)

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Evaluate the best model on the test set
best_model = grid_result.best_estimator_
y_pred = best_model.predict(X_test)
auc_score = roc_auc_score(y_test, y_pred)
print("Test AUC Score: ", auc_score)

best_params = grid_result.best_params_
print("Best Parameters:", best_params)

def create_best_model():
    input_layer = Input(shape=(X_train.shape[1],))
    x = Dense(best_params['model__neurons'])(input_layer)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(best_params['model__dropout_rate'])(x)
    output_layer = Dense(1, activation='sigmoid')(x)

    # Choose optimizer with the best learning rate
    if best_params['model__optimizer'] == 'adam':
        opt = Adam(learning_rate=best_params['model__learning_rate'])
    elif best_params['model__optimizer'] == 'sgd':
        opt = SGD(learning_rate=best_params['model__learning_rate'])

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create and train the best model
best_model = create_best_model()
best_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=1)

# Evaluate the best model on the test set
y_pred = best_model.predict(X_test)
auc_score_retest = roc_auc_score(y_test, y_pred)
print("Retest AUC Score: ", auc_score_retest)

with open('churning_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)

with open('standard_scaler.pkl', 'wb') as file:
    pickle.dump(sc, file)

with open('onehot_encoder.pkl', 'wb') as file:
    pickle.dump(encoder, file)

with open('label_encoder.pkl', 'wb') as file:
    pickle.dump(label_encoder, file)

from google.colab import files
files.download('churning_model.pkl')
files.download('standard_scaler.pkl')
files.download('onehot_encoder.pkl')
files.download('label_encoder.pkl')